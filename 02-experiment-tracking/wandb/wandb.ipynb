{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WANDB_NOTEBOOK_NAME=wandb-homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/deltasmith/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to your W&B account\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Install the Package\n",
    "To get started with Weights & Biases you'll need to install the appropriate Python package.\n",
    "\n",
    "For this we recommend creating a separate Python environment, for example, you can use conda environments, and then install the package there with pip or conda.\n",
    "\n",
    "Following are the libraries you need to install:\n",
    "```bash\n",
    "pandas\n",
    "matplotlib\n",
    "scikit-learn\n",
    "pyarrow\n",
    "wandb\n",
    "```\n",
    "Once you installed the package, run the command `wandb --version` and check the output.\n",
    "\n",
    "**What's the version that you have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb, version 0.15.3\n"
     ]
    }
   ],
   "source": [
    "!wandb --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Download and preprocess the data\n",
    "\n",
    "We'll use the Green Taxi Trip Records dataset to predict the amount of tips for each trip.\n",
    "\n",
    "Download the data for January, February and March 2022 in parquet format from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "\n",
    "**Tip:** In case you're on [GitHub Codespaces](https://github.com/features/codespaces) or [gitpod.io](https://gitpod.io), you can open up the terminal and run the following commands to download the data:\n",
    "\n",
    "```shell\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
    "```\n",
    "\n",
    "Use the script `preprocess_data.py` located in the folder [`homework-wandb`](homework-wandb) to preprocess the data.\n",
    "\n",
    "The script will:\n",
    "\n",
    "* initialize a Weights & Biases run.\n",
    "* load the data from the folder `<TAXI_DATA_FOLDER>` (the folder where you have downloaded the data),\n",
    "* fit a `DictVectorizer` on the training set (January 2022 data),\n",
    "* save the preprocessed datasets and the `DictVectorizer` to your Weights & Biases dashboard as an artifact of type `preprocessed_dataset`.\n",
    "\n",
    "Your task is to download the datasets and then execute this command:\n",
    "\n",
    "```bash\n",
    "python preprocess_data.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --raw_data_path <TAXI_DATA_FOLDER> \\\n",
    "  --dest_path ./output\n",
    "```\n",
    "\n",
    "Tip: go to `02-experiment-tracking/homework-wandb/` folder before executing the command and change the value of `<WANDB_PROJECT_NAME>` to the name of your Weights & Biases project, `<WANDB_USERNAME>` to your Weights & Biases username, and `<TAXI_DATA_FOLDER>` to the location where you saved the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/Users/deltasmith/.wget-hsts'. HSTS will be disabled.\n",
      "--2023-06-06 14:03:23--  https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
      "Resolviendo d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:200c:a00:b:20a5:b140:21, 2600:9000:200c:a600:b:20a5:b140:21, 2600:9000:200c:2600:b:20a5:b140:21, ...\n",
      "Conectando con d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)[2600:9000:200c:a00:b:20a5:b140:21]:443... conectado.\n",
      "Petici√≥n HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 1254291 (1.2M) [binary/octet-stream]\n",
      "Grabando a: ¬´green_tripdata_2022-01.parquet¬ª\n",
      "\n",
      "green_tripdata_2022 100%[===================>]   1.20M  4.41MB/s    en 0.3s    \n",
      "\n",
      "2023-06-06 14:03:24 (4.41 MB/s) - ¬´green_tripdata_2022-01.parquet¬ª guardado [1254291/1254291]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/Users/deltasmith/.wget-hsts'. HSTS will be disabled.\n",
      "--2023-06-06 14:03:24--  https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
      "Resolviendo d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:200c:b600:b:20a5:b140:21, 2600:9000:200c:a600:b:20a5:b140:21, 2600:9000:200c:1800:b:20a5:b140:21, ...\n",
      "Conectando con d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)[2600:9000:200c:b600:b:20a5:b140:21]:443... conectado.\n",
      "Petici√≥n HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 1428262 (1.4M) [binary/octet-stream]\n",
      "Grabando a: ¬´green_tripdata_2022-02.parquet¬ª\n",
      "\n",
      "green_tripdata_2022 100%[===================>]   1.36M  4.46MB/s    en 0.3s    \n",
      "\n",
      "2023-06-06 14:03:25 (4.46 MB/s) - ¬´green_tripdata_2022-02.parquet¬ª guardado [1428262/1428262]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/Users/deltasmith/.wget-hsts'. HSTS will be disabled.\n",
      "--2023-06-06 14:03:25--  https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
      "Resolviendo d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:200c:2600:b:20a5:b140:21, 2600:9000:200c:a600:b:20a5:b140:21, 2600:9000:200c:1800:b:20a5:b140:21, ...\n",
      "Conectando con d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)[2600:9000:200c:2600:b:20a5:b140:21]:443... conectado.\n",
      "Petici√≥n HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 1615562 (1.5M) [binary/octet-stream]\n",
      "Grabando a: ¬´green_tripdata_2022-03.parquet¬ª\n",
      "\n",
      "green_tripdata_2022 100%[===================>]   1.54M  5.10MB/s    en 0.3s    \n",
      "\n",
      "2023-06-06 14:03:26 (5.10 MB/s) - ¬´green_tripdata_2022-03.parquet¬ª guardado [1615562/1615562]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_DATA_FOLDER = \"./\"\n",
    "WANDB_PROJECT_NAME = \"wandb-homework\"\n",
    "WANDB_USERNAME = \"carloslme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarloslme\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/deltasmith/mlops-zoomcamp/02-experiment-tracking/wandb/wandb/run-20230606_141203-hhew05nw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeft-night-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/carloslme/wandb-homework\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/carloslme/wandb-homework/runs/hhew05nw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_data.py \\\n",
    "  --wandb_project {WANDB_PROJECT_NAME} \\\n",
    "  --wandb_entity {WANDB_USERNAME} \\\n",
    "  --raw_data_path {TAXI_DATA_FOLDER} \\\n",
    "  --dest_path ./output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you navigate to the `Files` tab of your artifact on your Weights & Biases page, **what's the size of the saved `DictVectorizer` file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 153.66 KB\n"
     ]
    }
   ],
   "source": [
    "def get_file_size(file_path):\n",
    "    size_in_bytes = os.path.getsize(file_path)\n",
    "    return size_in_bytes / 1000\n",
    "\n",
    "# Usage\n",
    "file_path = './output/dv.pkl'\n",
    "file_size_in_kb = get_file_size(file_path)\n",
    "print(f\"File size: {file_size_in_kb} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
